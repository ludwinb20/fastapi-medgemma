[
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "UploadFile",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "File",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Request",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "AutoProcessor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "UnidentifiedImageError",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "firebase_admin",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "firebase_admin",
        "description": "firebase_admin",
        "detail": "firebase_admin",
        "documentation": {}
    },
    {
        "label": "auth",
        "importPath": "firebase_admin",
        "description": "firebase_admin",
        "isExtraImport": true,
        "detail": "firebase_admin",
        "documentation": {}
    },
    {
        "label": "credentials",
        "importPath": "firebase_admin",
        "description": "firebase_admin",
        "isExtraImport": true,
        "detail": "firebase_admin",
        "documentation": {}
    },
    {
        "label": "AnalysisResult",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class AnalysisResult(BaseModel):\n    result: str\n    model: str = \"medgemma-4b-it\"\n    processing_time: Optional[float] = None\n# Nuevos modelos para los endpoints\nclass TextProcessRequest(BaseModel):\n    prompt: str\n    context: Optional[str] = None\nclass ImageProcessRequest(BaseModel):\n    imageDataUri: str",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "TextProcessRequest",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class TextProcessRequest(BaseModel):\n    prompt: str\n    context: Optional[str] = None\nclass ImageProcessRequest(BaseModel):\n    imageDataUri: str\n    prompt: str\nclass ProcessResponse(BaseModel):\n    response: str\n    tokens_used: int\n    success: bool",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "ImageProcessRequest",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class ImageProcessRequest(BaseModel):\n    imageDataUri: str\n    prompt: str\nclass ProcessResponse(BaseModel):\n    response: str\n    tokens_used: int\n    success: bool\n# Cargar el modelo (con manejo de errores)\ntry:\n    HF_TOKEN = os.getenv(\"HF_TOKEN\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "ProcessResponse",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class ProcessResponse(BaseModel):\n    response: str\n    tokens_used: int\n    success: bool\n# Cargar el modelo (con manejo de errores)\ntry:\n    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n    if not HF_TOKEN:\n        raise ValueError(\"HF_TOKEN no está configurado\")\n    logger.info(\"Cargando modelo MedGemma...\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "get_system_prompt",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def get_system_prompt() -> str:\n    return os.getenv(\"SYSTEM_PROMPT\", DEFAULT_SYSTEM_PROMPT)\ndef clean_response(full_response, prompt):\n    \"\"\"Limpia la respuesta removiendo el prompt original y repeticiones\"\"\"\n    # Buscar el último token de asistente en el prompt\n    assistant_markers = [\"<|im_start|>assistant\", \"<|im_end|>\", \"<|im_start|>user\", \"<|im_end|>\"]\n    # Intentar diferentes estrategias de limpieza\n    cleaned = full_response\n    # Estrategia 1: Buscar después del último marcador de asistente\n    for marker in assistant_markers:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "clean_response",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def clean_response(full_response, prompt):\n    \"\"\"Limpia la respuesta removiendo el prompt original y repeticiones\"\"\"\n    # Buscar el último token de asistente en el prompt\n    assistant_markers = [\"<|im_start|>assistant\", \"<|im_end|>\", \"<|im_start|>user\", \"<|im_end|>\"]\n    # Intentar diferentes estrategias de limpieza\n    cleaned = full_response\n    # Estrategia 1: Buscar después del último marcador de asistente\n    for marker in assistant_markers:\n        if marker in prompt:\n            parts = prompt.split(marker)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "generate_stream_response",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def generate_stream_response(model, processor, formatted_prompt, max_new_tokens=500):\n    \"\"\"Genera respuesta en streaming real usando TextIteratorStreamer\"\"\"\n    from transformers import TextIteratorStreamer\n    from threading import Thread\n    # Procesar con el modelo\n    inputs = processor(\n        text=formatted_prompt,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "logger = logging.getLogger(__name__)\napp = FastAPI(title=\"Medical Image Analysis API\", version=\"1.0\")\nDEFAULT_SYSTEM_PROMPT = (\n    \"Eres LucasMed, un asistente médico de IA.\\n\"\n    \"- Responde SIEMPRE en español.\\n\"\n    \"- Sé claro, profesional y conciso.\\n\"\n    \"- Responde solo al último mensaje del usuario usando el contexto si existe.\\n\"\n    \"- No uses el formato 'input:'/'output:'.\\n\"\n    \"- Incluye advertencias de seguridad solo cuando sea relevante.\"\n    \"- Como parte del contexto, vas a recibir mensajes enviados por el usuario y mensajes enviados por el asistente. No repitas respuestas del asistente, ni redundes en ellas.\"",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "app = FastAPI(title=\"Medical Image Analysis API\", version=\"1.0\")\nDEFAULT_SYSTEM_PROMPT = (\n    \"Eres LucasMed, un asistente médico de IA.\\n\"\n    \"- Responde SIEMPRE en español.\\n\"\n    \"- Sé claro, profesional y conciso.\\n\"\n    \"- Responde solo al último mensaje del usuario usando el contexto si existe.\\n\"\n    \"- No uses el formato 'input:'/'output:'.\\n\"\n    \"- Incluye advertencias de seguridad solo cuando sea relevante.\"\n    \"- Como parte del contexto, vas a recibir mensajes enviados por el usuario y mensajes enviados por el asistente. No repitas respuestas del asistente, ni redundes en ellas.\"\n)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "DEFAULT_SYSTEM_PROMPT",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "DEFAULT_SYSTEM_PROMPT = (\n    \"Eres LucasMed, un asistente médico de IA.\\n\"\n    \"- Responde SIEMPRE en español.\\n\"\n    \"- Sé claro, profesional y conciso.\\n\"\n    \"- Responde solo al último mensaje del usuario usando el contexto si existe.\\n\"\n    \"- No uses el formato 'input:'/'output:'.\\n\"\n    \"- Incluye advertencias de seguridad solo cuando sea relevante.\"\n    \"- Como parte del contexto, vas a recibir mensajes enviados por el usuario y mensajes enviados por el asistente. No repitas respuestas del asistente, ni redundes en ellas.\"\n)\ndef get_system_prompt() -> str:",
        "detail": "main",
        "documentation": {}
    }
]