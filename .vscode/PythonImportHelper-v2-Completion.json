[
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "UploadFile",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "File",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Request",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "StreamingResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "AutoProcessor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "UnidentifiedImageError",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "firebase_admin",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "firebase_admin",
        "description": "firebase_admin",
        "detail": "firebase_admin",
        "documentation": {}
    },
    {
        "label": "auth",
        "importPath": "firebase_admin",
        "description": "firebase_admin",
        "isExtraImport": true,
        "detail": "firebase_admin",
        "documentation": {}
    },
    {
        "label": "credentials",
        "importPath": "firebase_admin",
        "description": "firebase_admin",
        "isExtraImport": true,
        "detail": "firebase_admin",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "AnalysisResult",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class AnalysisResult(BaseModel):\n    result: str\n    model: str = \"medgemma-4b-it\"\n    processing_time: Optional[float] = None\n# Nuevos modelos para los endpoints\nclass TextProcessRequest(BaseModel):\n    prompt: str\n    context: Optional[str] = None\nclass ImageProcessRequest(BaseModel):\n    imageDataUri: str",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "TextProcessRequest",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class TextProcessRequest(BaseModel):\n    prompt: str\n    context: Optional[str] = None\nclass ImageProcessRequest(BaseModel):\n    imageDataUri: str\n    prompt: str\nclass ProcessResponse(BaseModel):\n    response: str\n    tokens_used: int\n    success: bool",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "ImageProcessRequest",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class ImageProcessRequest(BaseModel):\n    imageDataUri: str\n    prompt: str\nclass ProcessResponse(BaseModel):\n    response: str\n    tokens_used: int\n    success: bool\n# Cargar el modelo (con manejo de errores)\ntry:\n    HF_TOKEN = os.getenv(\"HF_TOKEN\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "ProcessResponse",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class ProcessResponse(BaseModel):\n    response: str\n    tokens_used: int\n    success: bool\n# Cargar el modelo (con manejo de errores)\ntry:\n    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n    if not HF_TOKEN:\n        raise ValueError(\"HF_TOKEN no está configurado\")\n    logger.info(\"Cargando modelo MedGemma...\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "clean_response",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def clean_response(full_response, prompt):\n    \"\"\"Limpia la respuesta removiendo el prompt original\"\"\"\n    # Buscar el último token de asistente en el prompt\n    assistant_markers = [\"<|im_start|>assistant\", \"<|im_end|>\", \"<|im_start|>user\", \"<|im_end|>\"]\n    # Intentar diferentes estrategias de limpieza\n    cleaned = full_response\n    # Estrategia 1: Buscar después del último marcador de asistente\n    for marker in assistant_markers:\n        if marker in prompt:\n            parts = prompt.split(marker)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "generate_stream_response",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def generate_stream_response(model, processor, formatted_prompt, max_new_tokens=500):\n    \"\"\"Genera respuesta en streaming\"\"\"\n    # Procesar con el modelo\n    inputs = processor(\n        text=formatted_prompt,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=2048\n    ).to(\"cuda\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "logger = logging.getLogger(__name__)\napp = FastAPI(title=\"Medical Image Analysis API\", version=\"1.0\")\ndef clean_response(full_response, prompt):\n    \"\"\"Limpia la respuesta removiendo el prompt original\"\"\"\n    # Buscar el último token de asistente en el prompt\n    assistant_markers = [\"<|im_start|>assistant\", \"<|im_end|>\", \"<|im_start|>user\", \"<|im_end|>\"]\n    # Intentar diferentes estrategias de limpieza\n    cleaned = full_response\n    # Estrategia 1: Buscar después del último marcador de asistente\n    for marker in assistant_markers:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "app = FastAPI(title=\"Medical Image Analysis API\", version=\"1.0\")\ndef clean_response(full_response, prompt):\n    \"\"\"Limpia la respuesta removiendo el prompt original\"\"\"\n    # Buscar el último token de asistente en el prompt\n    assistant_markers = [\"<|im_start|>assistant\", \"<|im_end|>\", \"<|im_start|>user\", \"<|im_end|>\"]\n    # Intentar diferentes estrategias de limpieza\n    cleaned = full_response\n    # Estrategia 1: Buscar después del último marcador de asistente\n    for marker in assistant_markers:\n        if marker in prompt:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "test_prompt_cleaning",
        "kind": 2,
        "importPath": "test-prompt-cleaning",
        "description": "test-prompt-cleaning",
        "peekOfCode": "def test_prompt_cleaning():\n    \"\"\"Prueba la limpieza de prompts específicos\"\"\"\n    print(\"🧪 Probando limpieza de prompts...\")\n    url = \"http://localhost:8000/api/process-text\"\n    # Datos de prueba (el caso problemático)\n    data = {\n        \"prompt\": \"Eres LucasMed, un asistente médico de IA en un chat con un doctor. Usa el contexto de los últimos mensajes para dar una respuesta precisa y útil.\\n\\nHistorial de mensajes:\\nai: Lo siento, no pude procesar tu solicitud. Intenta nuevamente más tarde.\\nuser: hola\\nuser: me duele la cabeza\\n\\nResponde al último mensaje del usuario de la forma más útil y profesional posible. Si hay imágenes, tenlas en cuenta en tu análisis.\",\n        \"context\": \"ai: Lo siento, no pude procesar tu solicitud. Intenta nuevamente más tarde.\\nuser: hola\\nuser: me duele la cabeza\"\n    }\n    try:",
        "detail": "test-prompt-cleaning",
        "documentation": {}
    },
    {
        "label": "test_simple_prompt",
        "kind": 2,
        "importPath": "test-prompt-cleaning",
        "description": "test-prompt-cleaning",
        "peekOfCode": "def test_simple_prompt():\n    \"\"\"Prueba con un prompt simple\"\"\"\n    print(\"\\n🧪 Probando prompt simple...\")\n    url = \"http://localhost:8000/api/process-text\"\n    data = {\n        \"prompt\": \"Hola, ¿cómo estás?\",\n        \"context\": \"Eres un asistente amigable\"\n    }\n    try:\n        response = requests.post(",
        "detail": "test-prompt-cleaning",
        "documentation": {}
    },
    {
        "label": "test_text_streaming",
        "kind": 2,
        "importPath": "test-streaming",
        "description": "test-streaming",
        "peekOfCode": "def test_text_streaming():\n    \"\"\"Prueba el streaming de texto\"\"\"\n    print(\"🧪 Probando streaming de texto...\")\n    url = \"http://localhost:8000/api/process-text-stream\"\n    # Datos de prueba\n    data = {\n        \"prompt\": \"Explica qué es la diabetes de manera simple\",\n        \"context\": \"Eres un asistente médico que explica conceptos médicos de forma clara\"\n    }\n    try:",
        "detail": "test-streaming",
        "documentation": {}
    },
    {
        "label": "test_image_streaming",
        "kind": 2,
        "importPath": "test-streaming",
        "description": "test-streaming",
        "peekOfCode": "def test_image_streaming():\n    \"\"\"Prueba el streaming de imagen\"\"\"\n    print(\"\\n🧪 Probando streaming de imagen...\")\n    url = \"http://localhost:8000/api/process-image-stream\"\n    # Crear una imagen de prueba simple (1x1 pixel)\n    import base64\n    from PIL import Image\n    import io\n    # Crear imagen de prueba\n    img = Image.new('RGB', (100, 100), color='red')",
        "detail": "test-streaming",
        "documentation": {}
    }
]